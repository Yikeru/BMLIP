{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c5dc26",
   "metadata": {},
   "source": [
    "### Generative Classification\n",
    "\n",
    "- **[1]** You have a machine that measures property $x$, the \"orangeness\" of liquids. You wish to discriminate between $C_1 = \\text{`Fanta'}$ and $C_2 = \\text{`Orangina'}$. It is known that\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x|C_1) &= \\begin{cases} 10 & 1.0 \\leq x \\leq 1.1\\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "p(x|C_2) &= \\begin{cases} 200(x - 1) & 1.0 \\leq x \\leq 1.1\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "The prior probabilities $p(C_1) = 0.6$ and $p(C_2) = 0.4$ are also known from experience.       \n",
    "  (a) (##) A \"Bayes Classifier\" is given by\n",
    "  \n",
    "$$ \\text{Decision} = \\begin{cases} C_1 & \\text{if } p(C_1|x)>p(C_2|x) \\\\\n",
    "                               C_2 & \\text{otherwise}\n",
    "                 \\end{cases}\n",
    "$$\n",
    "\n",
    "Derive the optimal Bayes classifier.      \n",
    "  (b) (###) The probability of making the wrong decision, given $x$, is\n",
    "  \n",
    "$$\n",
    "p(\\text{error}|x)= \\begin{cases} p(C_1|x) & \\text{if we decide $C_2$}\\\\\n",
    "    p(C_2|x) & \\text{if we decide $C_1$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Compute the **total** error probability  $p(\\text{error})$ for the Bayes classifier in this example.\n",
    "\n",
    "> (a) We choose $C_1$ if $p(C_1|x)/p(C_2|x) > 1$. This condition can be worked out as\n",
    "$$\n",
    "\\frac{p(C_1|x)}{p(C_2|x)} = \\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)} = \\frac{10 \\times 0.6}{200(x-1)\\times 0.4}>1\n",
    "$$\n",
    "which evaluates to choosing\n",
    "$$\\begin{align*}\n",
    "C_1 &\\quad \\text{ if $1.0\\leq x < 1.075$}\\\\ \n",
    "C_2 &\\quad \\text{ if $1.075 \\leq x \\leq 1.1$ }\n",
    "\\end{align*}$$\n",
    "The probability that $x$ falls outside the interval $[1.0,1.1]$ is zero.       \n",
    "> (b) The total probability of error $p(\\text{error})=\\int_x p(\\text{error}|x)p(x) \\mathrm{d}{x}$. We can work this out as\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\text{error}) &= \\int_x p(\\text{error}|x)p(x)\\mathrm{d}{x}\\\\\n",
    "&= \\int_{1.0}^{1.075} p(C_2|x)p(x) \\mathrm{d}{x} + \\int_{1.075}^{1.1} p(C_1|x)p(x) \\mathrm{d}{x}\\\\\n",
    "&= \\int_{1.0}^{1.075} p(x|C_2)p(C_2) \\mathrm{d}{x} + \\int_{1.075}^{1.1} p(x|C_1)p(C_1) \\mathrm{d}{x}\\\\\n",
    "&= \\int_{1.0}^{1.075}0.4\\cdot 200(x-1) \\mathrm{d}{x} + \\int_{1.075}^{1.1} 0.6\\cdot 10 \\mathrm{d}{x}\\\\\n",
    "&=80\\cdot[x^2/2-x]_{1.0}^{1.075} + 6\\cdot[x]_{1.075}^{1.1}\\\\\n",
    "&=0.225 + 0.15\\\\\n",
    "&=0.375\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "- **[2]** (#) (see Bishop exercise 4.8): Using (4.57) and (4.58) (from Bishop's book), derive the result (4.65) for the posterior class probability in the two-class generative model with Gaussian densities, and verify the results (4.66) and (4.67) for the parameters $w$ and $w0$. \n",
    "\n",
    "> Substitute 4.64 into 4.58 to get  \n",
    "$$\\begin{align*}\n",
    "a &= \\log \\left( \\frac{ \\frac{1}{(2\\pi)^{D/2}} \\cdot \\frac{1}{|\\Sigma|^{1/2}} \\cdot \\exp\\left( -\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1)\\right) \\cdot p(C_1)}{\\frac{1}{(2\\pi)^{D/2}} \\cdot \\frac{1}{|\\Sigma|^{1/2}}\\cdot  \\exp\\left( -\\frac{1}{2}(x-\\mu_2)^T \\Sigma^{-1} (x-\\mu_2)\\right) \\cdot p(C_2)}\\right) \\\\\n",
    "&= \\log \\left(  \\exp\\left(-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1) + \\frac{1}{2}(x-\\mu_2)^T \\Sigma^{-1} (x-\\mu_2) \\right) \\right) + \\log \\frac{p(C_1)}{p(C_2)} \\\\\n",
    "&= ... \\\\\n",
    "&=( \\mu_1-\\mu_2)^T\\Sigma^{-1}x - 0.5\\left(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_2^T\\Sigma^{-1} \\mu_2\\right)+ \\log \\frac{p(C_1)}{p(C_2)} \n",
    "\\end{align*}$$ \n",
    "Substituting this into the right-most form of (4.57) we obtain (4.65), with $w$ and $w0$ given by (4.66) and (4.67), respectively.\n",
    "\n",
    "- **[3]** (###) (see Bishop exercise 4.9). \n",
    "> The Log-likelihood is given by\n",
    "$$\\log p(\\{\\phi_n,t,n\\} | \\{\\pi_k\\}) = \\sum_n \\sum_k t_{nk}\\left(\\log p(\\phi_n|C_k) + \\log \\pi_k\\right)\\,.$$ Using the method of Lagrange multipliers (Bishop app.E), we augment the log-likelihood with the constraint and obtain the Lagrangian $$\\log p(\\{\\phi_n,t_{nk}\\} | \\{\\pi_k\\})+\\lambda \\left(\\sum_k \\pi_k -1  \\right)\\,.$$ In order to maximize, we set the derivative with respect to $\\pi_k$ equal to zero and obtain \n",
    "$$\\begin{align*}\n",
    "\\sum_n \\frac{t_{nk}}{\\pi_k}+\\lambda &=0 \\\\\n",
    "-\\pi_k\\lambda &=\\sum_n t_{nk} = N_k \\\\\n",
    "-\\lambda \\sum_k \\pi_k &= \\sum_k \\sum_n t_{nk} \\\\\n",
    "\\lambda &= -N \n",
    "\\end{align*}$$\n",
    "\n",
    "- **[4]** (##) (see Bishop exercise 4.10).    \n",
    "> We can write the log-likelihood as\n",
    "$$\\begin{align*}\n",
    "\\log p(\\{\\phi_n,t_n\\}|\\{\\pi_k\\}) \\propto -0.5\\sum_n\\sum_kt_{nk}\\left(\\log |\\Sigma|+(\\phi_n-\\mu_k)^T\\Sigma^{-1}(\\phi-\\mu)\\right)\n",
    "\\end{align*}$$\n",
    "The derivatives of the likelihood with respect to mean and shared covariance are respectively\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\mu_k}\\log p(\\{\\phi_n,t_n\\}|\\{\\pi_k\\}) &= \\sum_n\\sum_k t_{nk}\\Sigma^{-1}\\left(\\phi_n-\\mu_k\\right) = 0 \\\\\n",
    "\\sum_n t_{nk}\\left(\\phi_n-\\mu_k\\right))&=0 \\\\\n",
    "\\mu_k &= \\frac{1}{N_k}\\sum_n t_{nk}\\phi_n \\\\\n",
    "\\nabla_{\\Sigma}\\log p(\\{\\phi_n,t_n\\}|\\{\\pi_k\\})&=\\sum_n\\sum_k t_{nk}\\left(\\Sigma - (\\phi_n-\\mu_k)(\\phi_n-\\mu_k)^T\\right) = 0 \\\\\n",
    "\\sum_n\\sum_k t_{nk}\\Sigma &= \\sum_n\\sum_k t_{nk}(\\phi_n-\\mu_k)(\\phi_n-\\mu_k)^T \\\\\n",
    "\\Sigma &=  \\frac{1}{N}\\sum_k\\sum_n t_{nk}(\\phi_n-\\mu_k)(\\phi_n-\\mu_k)^T\n",
    "\\end{align*}$$\n",
    "\n",
    "<!----\n",
    "- **[2]** Consider a given data set $D=\\{(x_1,y_1),\\ldots,(x_N,y_N)\\}$ where $x_n \\in \\mathbb{R}^M$ are input features and $y_n \\in \\{0,1\\}$ are given class labels.    \n",
    "  (a) (#) Write down the generative model for this classification task using a Gaussian likelihood with Bernoulli priors for class labels.  \n",
    "  (b) How do you compute the posterior distribution for class labels, given a new input $x_\\bullet$, ie, how do you compute $p(y_\\bullet|x_\\bullet,D)$?      \n",
    "  (c) (##) Work out the likelihood function for the parameters.     \n",
    "  (d) (###) Derive the Maximum Likelihood estimates for the parameters by the gradient of the likelihood to zero.\n",
    "  \n",
    "- **[3]** Refer to [Bishop's PRML book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) for the following exercises. Chapter 4: 4.8, 4.9, 4.10.\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d7b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
